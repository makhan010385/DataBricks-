{"version":"NotebookV1","origId":697221008599095,"name":"009. Write DataFrame","language":"python","commands":[{"version":"CommandV1","origId":697221008599100,"guid":"1647d7ec-aa4e-498f-9cf0-8e405259b301","subtype":"command","commandType":"auto","position":5.0,"command":"#Write the file into Databricks Storage\nDf1.write.format(\"csv\").save(\"/FileStore/tables/PysparkNew\")","commandVersion":23,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709862931780,"submitTime":1709862931734,"finishTime":1709862935254,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"ef526497-0e5b-435b-ba3f-b5df5a2108b1"},{"version":"CommandV1","origId":697221008599101,"guid":"eaf6c0de-2823-4049-a00b-010ff7dfa3df","subtype":"command","commandType":"auto","position":6.0,"command":"Df1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").save()","commandVersion":3,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709866236349,"submitTime":1709866236307,"finishTime":1709866238284,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"5fe88e4a-156e-42ce-a367-001fc5acd452"},{"version":"CommandV1","origId":3383680411990053,"guid":"2243630f-4a93-4182-aca1-b93819dce630","subtype":"command","commandType":"auto","position":7.0,"command":"Df1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").save()\n","commandVersion":2,"state":"error","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/tables/PysparkNew2 already exists.","errorTraceType":"ansi","error":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3383680411990053>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mDf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables/PysparkNew2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/PysparkNew2 already exists.","errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709866451134,"submitTime":1709866451125,"finishTime":1709866451563,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"13550214-ecdd-488a-b6c3-b0ad8871a81c"},{"version":"CommandV1","origId":3383680411990054,"guid":"5a0695d9-f117-44e3-9478-e495463e45a6","subtype":"command","commandType":"auto","position":8.0,"command":"#how to handle this types of exceptions with differents modes\nhelp(Df1.write)","commandVersion":28,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"Help on DataFrameWriter in module pyspark.sql.readwriter object:\n\nclass DataFrameWriter(OptionUtils)\n |  DataFrameWriter(df: 'DataFrame')\n |  \n |  Interface used to write a :class:`DataFrame` to external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n |  to access this.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  .. versionchanged:: 3.4.0\n |      Support Spark Connect.\n |  \n |  Method resolution order:\n |      DataFrameWriter\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: 'DataFrame')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Buckets the output by the given columns. If specified,\n |      the output is laid out on the file system similar to Hive's bucketing scheme,\n |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      numBuckets : int\n |          the number of buckets to save\n |      col : str, list or tuple\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Notes\n |      -----\n |      Applicable for file-based data sources in combination with\n |      :py:meth:`DataFrameWriter.saveAsTable`.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n |  \n |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n |              exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a CSV file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file\n |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n |      ...     df.write.csv(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n |      +---+----+\n |      |age|name|\n |      +---+----+\n |      |100|null|\n |      +---+----+\n |  \n |  format(self, source: str) -> 'DataFrameWriter'\n |      Specifies the underlying output data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.format('parquet')\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format('parquet').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n |      Inserts the content of the :class:`DataFrame` to the specified table.\n |      \n |      It requires that the schema of the :class:`DataFrame` is the same as the\n |      schema of the table.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      overwrite : bool, optional\n |          If true, overwrites existing data. Disabled by default\n |      \n |      Notes\n |      -----\n |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n |      the column names and just uses position-based resolution.\n |      \n |      Examples\n |      --------\n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> df = spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... )\n |      >>> df.write.saveAsTable(\"tblA\")\n |      \n |      Insert the data into 'tblA' table but with different column names.\n |      \n |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Name of the table in the external database.\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      properties : dict\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |  \n |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n |      Saves the content of the :class:`DataFrame` in JSON format\n |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n |      specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.json(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format(\"json\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n |      Specifies the behavior when data or table already exists.\n |      \n |      Options include:\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Examples\n |      --------\n |      Raise an error when writing to an existing path.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n |      Traceback (most recent call last):\n |          ...\n |      ...AnalysisException: ...\n |      \n |      Write a Parquet file back with various options, and read it back.\n |      \n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Overwrite the path with a new Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Append another DataFrame into the Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |120|Takuya Ueshin|\n |      |100| Hyukjin Kwon|\n |      +---+-------------+\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds an output option for the underlying data source.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      key : str\n |          The key for the option to set.\n |      value\n |          The value for the option to set.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' with writing a CSV file.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds output options for the underlying data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      **options : dict\n |          The dictionary of string keys and primitive-type values.\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1).write.option(\"key\", \"value\")\n |      <...readwriter.DataFrameWriter object ...>\n |      \n |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n |      \n |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n |      >>> schema = StructType([\n |      ...     StructField(\"age\",IntegerType(),True),\n |      ...     StructField(\"name\",StringType(),True),\n |      ... ])\n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n |      ...     # and 'header' option set to `True`.\n |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n |      ...         \"overwrite\").format(\"csv\").save(d)\n |      ...\n |      ...     # Read the CSV file as a DataFrame.\n |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a ORC file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a ORC file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.orc(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"orc\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.parquet(d, mode=\"overwrite\")\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.format(\"parquet\").load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n |      Partitions the output by the given columns on the file system.\n |      \n |      If specified, the output is laid out on the file system similar\n |      to Hive's partitioning scheme.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      cols : str or list\n |          name of columns\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n |      \n |      >>> import tempfile\n |      >>> import os\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n |      ...\n |      ...     # Read the Parquet file as a DataFrame.\n |      ...     spark.read.parquet(d).sort(\"age\").show()\n |      ...\n |      ...     # Read one partition as a DataFrame.\n |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n |      +---+-------------+\n |      |age|         name|\n |      +---+-------------+\n |      |100| Hyukjin Kwon|\n |      |120|Ruifeng Zheng|\n |      +---+-------------+\n |      +---+\n |      |age|\n |      +---+\n |      |100|\n |      +---+\n |  \n |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the contents of the :class:`DataFrame` to a data source.\n |      \n |      The data source is specified by the ``format`` and a set of ``options``.\n |      If ``format`` is not specified, the default data source configured by\n |      ``spark.sql.sources.default`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str, optional\n |          the path in a Hadoop supported file system\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : list, optional\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a JSON file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a JSON file\n |      ...     spark.createDataFrame(\n |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n |      ...\n |      ...     # Read the JSON file as a DataFrame.\n |      ...     spark.read.format('json').load(d).show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      +---+------------+\n |  \n |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the content of the :class:`DataFrame` as the specified table.\n |      \n |      In the case the table already exists, behavior of this function depends on the\n |      save mode, specified by the `mode` function (default to throwing an exception).\n |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n |      the same as that of the existing table.\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Notes\n |      -----\n |      When `mode` is `Append`, if there is an existing table, we will use the format and\n |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n |      doesn't need to be the same as that of the existing table. Unlike\n |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n |      column names to find the correct column positions.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          the table name\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n |      partitionBy : str or list\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      Creates a table from a DataFrame, and read it back.\n |      \n |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.saveAsTable(\"tblA\")\n |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n |  \n |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Sorts the output in each bucket by the given columns on the file system.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      col : str, tuple or list\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n |      \n |      >>> from pyspark.sql.functions import input_file_name\n |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n |      >>> spark.createDataFrame([\n |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n |      ...     schema=[\"age\", \"name\"]\n |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n |      >>> # Read the Parquet file as a DataFrame.\n |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n |      +---+------------+\n |      |age|        name|\n |      +---+------------+\n |      |100|Hyukjin Kwon|\n |      |120|Hyukjin Kwon|\n |      |140| Haejoon Lee|\n |      +---+------------+\n |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n |  \n |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the DataFrame in a text file at the specified path.\n |      The text files will be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      .. versionchanged:: 3.4.0\n |          Support Spark Connect.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          for the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      The DataFrame must have only one column that is of string type.\n |      Each row becomes a new line in the output file.\n |      \n |      Examples\n |      --------\n |      Write a DataFrame into a text file and read it back.\n |      \n |      >>> import tempfile\n |      >>> with tempfile.TemporaryDirectory() as d:\n |      ...     # Write a DataFrame into a text file\n |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n |      ...     df.write.mode(\"overwrite\").text(d)\n |      ...\n |      ...     # Read the text file as a DataFrame.\n |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n |      +---------+\n |      |alphabets|\n |      +---------+\n |      |        a|\n |      |        b|\n |      |        c|\n |      +---------+\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709866709316,"submitTime":1709866709285,"finishTime":1709866709499,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",30340]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"0776fd93-f5ac-49fe-8d0e-b9e1d33c86d1"},{"version":"CommandV1","origId":3383680411990055,"guid":"bda83fcc-a551-442c-a556-e9dc8466b28d","subtype":"command","commandType":"auto","position":9.0,"command":"Df1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").mode(\"append\").save()","commandVersion":5,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709866816396,"submitTime":1709866816384,"finishTime":1709866818260,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"b58aa508-cfe0-4ab4-ad19-73c276494a18"},{"version":"CommandV1","origId":3383680411990056,"guid":"c7018b16-22ac-4184-96b9-6ce681efeff3","subtype":"command","commandType":"auto","position":10.0,"command":"#we will get only one partfile after excecuting this commands\nDf1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").mode(\"overwrite\").save()","commandVersion":24,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709869918925,"submitTime":1709869918896,"finishTime":1709869921314,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"540be022-2ddd-4416-9341-0932e7e24a18"},{"version":"CommandV1","origId":3383680411990057,"guid":"e2ce483e-1554-4da6-b992-e611c7b074cf","subtype":"command","commandType":"auto","position":11.0,"command":"","commandVersion":0,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"96b48b62-51e5-48e2-9cd2-79f85233c626"},{"version":"CommandV1","origId":3383680411990058,"guid":"5902db19-fea3-454d-8c8c-f857715fa495","subtype":"command","commandType":"auto","position":10.5,"command":"#we path alreay exists that shows error\nDf1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").mode(\"error\").save()","commandVersion":13,"state":"error","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/tables/PysparkNew2 already exists.","errorTraceType":"ansi","error":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3383680411990058>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#we path alreay exists\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mDf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables/PysparkNew2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1395\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1393\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m   1394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1395\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1397\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/PysparkNew2 already exists.","errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709870072087,"submitTime":1709870072078,"finishTime":1709870072615,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"7d0f9690-59e1-4859-86dd-ae8b866edf09"},{"version":"CommandV1","origId":3383680411990059,"guid":"89353ce4-df4b-4e6e-a887-702d11301b7a","subtype":"command","commandType":"auto","position":10.75,"command":"#we path alreay exists that ignore using ignore mode\nDf1.write.format(\"csv\").option(\"path\",\"/FileStore/tables/PysparkNew2\").mode(\"ignore\").save()","commandVersion":10,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1709870143325,"submitTime":1709870143316,"finishTime":1709870143507,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"1c66d51f-a8bd-4e96-aa31-5d2181ff63d0"}],"dashboards":[],"guid":"a3d55c98-3ff1-4917-a8e4-fc49e51b90d7","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":4},"reposExportFormat":"SOURCE","environmentMetadata":null,"computePreferences":null,"inputWidgetPreferences":null}